# -*- coding: utf-8 -*-
"""Copy of All-TA-Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pBL6tEAWk39xFO8FUz1wVU4d0n-pJi6R

# PENDEKATAN HYBRID CNN GLOVE - SVM UNTUK ANALISIS SENTIMEN BERBASIS ASPEK PADA ULASAN APLIKASI BRIMO

### Import Global Package/Library
"""

# connect to google drive
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import shutil
import pickle
import os
from collections import Counter
from distutils.dir_util import copy_tree

"""# Data Scraping"""

# Instalation and import library untuk data scraping
!pip install google-play-scraper
from google_play_scraper import app
from google_play_scraper import Sort, reviews_all

# define variable configuration
app_name = 'brimo'
app_id = 'id.co.bri.brimo'
lang = 'id'
country = 'id'

# scraping the reviews using reviews_all function with parameter from variable that already define
reviews = reviews_all(
    f"{app_id}",
    sleep_milliseconds=0,
    lang=f"{lang}",
    country=f"{country}",
    sort=Sort.MOST_RELEVANT,
)

# create dataframe using pandas library to store the scraping result
df_reviews = pd.DataFrame(np.array(reviews),columns=['review'])
df_reviews = df_reviews.join(pd.DataFrame(df_reviews.pop('review').tolist()))


# show the first 5 data
df_reviews.head()

# save data to google drive with csv format
df_reviews.to_csv('/content/drive/My Drive/TA/review_brimo.csv', index=None, header=True)

df_ =df_reviews[['content','score','at','appVersion']]
df_.head(10)

df_.to_csv('/content/drive/My Drive/TA/review_brimo_4_kolom.csv', index=None, header=True)

"""# Text Pre-processing

### Import library untuk text-preprocessing
"""

# import library untuk text-preprocessing
import re  # Modul untuk bekerja dengan ekspresi reguler
from nltk.tokenize import word_tokenize  # Tokenisasi teks
from nltk.corpus import stopwords  # Daftar kata-kata berhenti dalam teks
import string

!pip install sastrawi
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory  # Stemming (penghilangan imbuhan kata) dalam bahasa Indonesia
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory  # Menghapus kata-kata berhenti dalam bahasa Indonesia

import nltk  # Import pustaka NLTK (Natural Language Toolkit).
nltk.download('punkt_tab') # Mengunduh dataset yang diperlukan untuk tokenisasi teks.
nltk.download('stopwords')  # Mengunduh dataset yang berisi daftar kata-kata berhenti (stopwords) dalam berbagai bahasa.



"""### Dictinary Typo"""

dic_singkatan = {
    "tba": "tiba",
    "tp" : "tapi",
    "tpi": "tapi",
    "sngt" : "sangat",
    "sy": "saya",
    "sya": "saya",
    "gk" :"gak",
    "tf": "transfer",
    "trferan" : "transferan",
    "transperan" : "transferan",
    "transper" : "transfer",
    "jozz": "bagus",
    "mantul": "bagus",
    "hr" : "hari",
    "sarat" : "syarat",
    "seikalu": "sekali",
    "sekalai": "sekali",
    "eror": "error",
    "dlm" : "dalam",
    "sgt": "sangat",
    "idh" : "sudah",
    "udh" : "sudah",
    "bosa" : "bisa",
    "pkai" : "pakai",
    "gk" : "enggak",
    "ngk" : "enggak",
    "ndak" : "tidak",
    "apk ": "aplikasi",
    "negri" : "negeri",
    "hp" : "handphone",
    "tdk" : "tidak",
    "sdh" : "sudah",
    "butih" : "bukti",
    "lgi" : "lagi",
    "lg" : "lagi",
    "mantafffff" : "mantap",
    "ufdate": "update",
    "truss" : "terus",
    "trs" : "terus",
    "jd" : "jadi",
    "mkin" : "makin",
    "yg" : "yang",
    "kk" : "kakak",
    "nmr" : "nomor",
    "mdr": "merchant discount rate",
    "poto" : "foto",
    "photo" : "foto",
    "sdgkan" : "sedangkan",
    "sdg" : "sedang",
    "koq" : "kok",
    "skrg" : "sekarang",
    "apps" : "aplikasi",
    "kintil" : "kontol",
    "blm" : "belum",
    "kalou" : "kalau",
    "unsurnem" : "username",
    "klou" : "kalau",
    "belm" : "belum",
    "pdhal" : "pada hal",
    "pd" : "pada",
    "tgl" :"tanggal",
    "knp" : "kenapa",
    "kenceng" : "cepat",
    "keneh" : "masih",
    "sarua" : "sama",
    "lemot" : "lambat",
    "rb" : "ribu",
    "lho" : "loh",
    "thx" : "terimakasih",
    "trx" : "terimakasih",
    "spt" : "seperti",
    "krna ": "karena",
    "bnget" : "banget",
    "sllu" : "selalu",
    "tdak" : "tidak",
    "dll" : "dan lain lain",
    "klw" : "kalau",
    "utk ": "untuk",
    "elor" : "error",
    "rudet" : "susah",
    "solsusi" : "solusi",
    "koata" : "kuota",
    "belu ": "belum",
    "yng" : "yang",
    "apdet ": "update",
    "mnurut" : "menurut",
    "bs" : "bisa",
    "skarang" : "sekarang",
    "pasilitasi" : "fasilitasi",
    "centr" : "center",
    "notif" : "notifikasi",
    "stiap" : "setiap",
    "kriditan" : "kreditan",
    "kridit" : "kredit",
    "blja" : "belanja",
    "pw" : "password",
    "sebelom" : "sebelum",
    "ferifikasi" : "verifikasi",
    "regis": "registrasi",
    "code ": "kode",
    "camera" : "kamera",
    "kren" : "keren",
    "mauntaf" : "mantap",
    "ggl" : "gagal",
    "luwar" : "luar",
    "biyasa": "biasa",
    "daptar" : "daftar",
    "pinger" : "finger",
    "fotongan" : "potongan",
    "memanbtu" : "membantu",
    "ditinenggakatkan" : "ditingkatkan",
    "teribulokir" : "terblokir",
    "pass" : "password",
    "jlk" : "jelek",
    "diperibuaiki" : "diperbaiki",
    "jg" : "juga",
    "teribuaik" : "terbaik",
    "penyaimpanan" : "penyimpanan",
    "berkirang" : "berkurang",
    "teribuaru" : "terbaru",
    "menyaediakan": "menyediakan",
    "peneribuitan" : "penerbitan",
    "sedanenggakan" : "sedangkan",
    "briva" : "brimo",
    "menyaelesaikan" : "menyelesaikan",
    "membanti" : "membantu",
    "daptar":"daftar",
    "mudahahahahah" : "mudah"
}

"""### Function Text-Preprocessing"""



def cleaningText(text):
    text = re.sub(r'@[A-Za-z0-9]+', '', text) # menghapus mention
    text = re.sub(r'#[A-Za-z0-9]+', '', text) # menghapus hashtag
    text = re.sub(r'RT[\s]', '', text) # menghapus RT
    text = re.sub(r"http\S+", '', text) # menghapus link
    text = re.sub(r'[0-9]+', '', text) # menghapus angka
    text = re.sub(r'[^\w\s]', ' ', text) # menghapus karakter selain huruf dan angka
    text = re.sub(r'(.)\1+', r'\1', text) # menghapus karakter perulang
    text = text.replace('\n', ' ') # mengganti baris baru dengan spasi
    text = text.translate(str.maketrans(' ', ' ', string.punctuation)) # menghapus semua tanda baca
    text = text.strip(' ') # menghapus karakter spasi dari kiri dan kanan teks
    return text

def casefoldingText(text): # Mengubah semua karakter dalam teks menjadi huruf kecil
    text = text.lower()
    return text

def tokenizingText(text): # Memecah atau membagi string, teks menjadi daftar token
    text = word_tokenize(text)
    return text

def filteringText(text): # Menghapus stopwords dalam teks
    listStopwords = set(stopwords.words('indonesian'))
    listStopwords1 = set(stopwords.words('english'))
    listStopwords.update(listStopwords1)
    listStopwords.update(['iya','yaa','gak','nya','na','sih','ku',"di","ga","ya","gaa","loh","kah","woi","woii","woy", "nga"])
    filtered = []
    for txt in text:
        if txt not in listStopwords:
            filtered.append(txt)
    text = filtered
    return text

def stemmingText(text): # Mengurangi kata ke bentuk dasarnya yang menghilangkan imbuhan awalan dan akhiran atau ke akar kata
    # Membuat objek stemmer
    factory = StemmerFactory()
    stemmer = factory.create_stemmer()

    # Memecah teks menjadi daftar kata
    words = text.split()

    # Menerapkan stemming pada setiap kata dalam daftar
    stemmed_words = [stemmer.stem(word) for word in words]

    # Menggabungkan kata-kata yang telah distem
    stemmed_text = ' '.join(stemmed_words)

    return stemmed_text

def toSentence(list_words): # Mengubah daftar kata menjadi kalimat
    sentence = ' '.join(word for word in list_words)
    return sentence

def normalization(text, slang_dict):
    words = text.split()

    # Check and normalize slang only if it's a standalone word
    def is_slang_word(word, slang_dict):
        return word in slang_dict

    def safe_normalize(word):
        # Avoid changing middle parts of valid words (e.g., "menbingungkan")
        for slang, norm in slang_dict.items():
            if slang in word and len(word) > len(slang):
                # Don't change word if slang is in the middle
                return word
        return slang_dict.get(word, word)  # Apply normalization only to full slang words

    # Normalize words
    normalized_words = [safe_normalize(word) for word in words]

    return ' '.join(normalized_words)

"""### Read Data"""

df = pd.read_csv('/content/drive/MyDrive/TA/review_brimo_4_kolom.csv')
df.head(10)

df.info()



"""## Missing Value Handling"""

print("Jumlah data null:\n",df.isnull().sum())
print("Jumlah data duplicat:",df.duplicated().sum())

df['appVersion'].fillna(0, inplace=True)

print("Jumlah data null:\n",df.isnull().sum())
print("Jumlah data duplicat:",df.duplicated().sum())

df.info()

"""### Distribusi Data"""

# change data type
df['content'] = df['content'].astype(str)
df['score'] = df['score'].astype(int)
df['at'] = pd.to_datetime(df['at'])

# Split coloumn at
df['month'] = df['at'].dt.strftime('%m')
df['year'] = df['at'].dt.strftime('%Y')

review_counts_by_year = df.groupby('year')['content'].count()

# Sort by year
review_counts_by_year = review_counts_by_year.sort_index()

# Create the chart
plt.figure(figsize=(10, 6))
plt.bar(review_counts_by_year.index, review_counts_by_year.values, color='#5ec962')
plt.xlabel('Tahun')
plt.ylabel('Jumlah Ulasan')
plt.title('Jumlah Ulasan Per Year')
plt.savefig('ulasan_per_tahun.png', dpi=1200, transparent=True, format='png')
plt.show()

# Group by year and month, and count 'content'
grouped = df.groupby(["year", "month"])["content"].count().unstack("year", fill_value=0)

# Plot the stacked bar chart
grouped.plot(kind="bar", stacked=True, figsize=(10, 6), colormap="viridis")

# Customize the plot
plt.title("Grafik Dari Ulasan Berdasarkan Tahun dan Bulan")
plt.xlabel("Bulan")
plt.ylabel("Jumlah Ulasan")
plt.xticks(rotation=45)
plt.legend(title="Tahun", bbox_to_anchor=(1.05, 1), loc="upper left")
plt.tight_layout()
plt.savefig('ulasan_per_tahun_bulan.png', dpi=1200, transparent=True, format='png')

# Show the plot
plt.show()

string_lengths = df['content'].astype(str).str.len()
plt.figure(figsize=(10, 6))
plt.hist(string_lengths, bins=50, color='#5ec962')  # Adjust bins as needed
plt.xlabel('Panjang Ulasan')
plt.ylabel('Jumlah')
plt.title(f'Distribusi dari Panjang Text pada Ulasan')
plt.savefig('distribusi_panjang_ulasan.png', dpi=1200, transparent=True, format='png')
plt.show()

"""## Cleaning Text & Case Folding"""

df['cleanText'] = df['content'].apply(cleaningText)
df['lowerText'] = df['cleanText'].apply(casefoldingText)
df.tail(3)

"""## Normalization"""

df['fix_typo'] = df['lowerText'].apply(lambda row: normalization(row, dic_singkatan))
df.tail(3)

"""## Tokenizing & Removal Stopwords"""

df['tokenText'] = df['fix_typo'].apply(tokenizingText)
df['filterText'] = df['tokenText'].apply(filteringText)
df.head(3)

df['text'] = df['filterText'].apply(toSentence)
df.head(3)

"""## Stemming

We Slicing Dataframe by year for not time comsuming the stemming process
"""

# slicing dta berdasarkan tahun
df_2022 = df[df['year'] == '2022']
df_2023 = df[df['year'] == '2023']
df_2024 = df[df['year'] == '2024']

print(f'shape 2022: {df_2022.shape}')
print(f'shape 2023: {df_2023.shape}')
print(f'shape 2024: {df_2024.shape}')
print(f'total rows 2022-2024: {df_2022.shape[0] + df_2023.shape[0] + df_2024.shape[0]}')

# membuat sample dari dataframe per tahun
df_2022_sample = df_2022.sample(frac=0.118)
df_2023_sample = df_2023.sample(frac=0.118)
df_2024_sample = df_2024.sample(frac=0.118)

print(f'shape 2022_sample: {df_2022_sample.shape}')
print(f'shape 2023_sample: {df_2023_sample.shape}')
print(f'shape 2024_sample: {df_2024_sample.shape}')
print(f'total rows 2022-2024_sample: {df_2022_sample.shape[0] + df_2023_sample.shape[0] + df_2024_sample.shape[0]}')

df_2022_sample['stemmingText'] = df_2022_sample['text'].apply(stemmingText)

df_2023_sample['stemmingText'] = df_2023_sample['text'].apply(stemmingText)

df_2024_sample['stemmingText'] = df_2024_sample['text'].apply(stemmingText)

"""### Save Result"""

df_2022_2024 = pd.concat([df_2022, df_2023, df_2024])

df_2022_2024_sample =pd.concat([df_2022_sample, df_2023_sample, df_2024_sample])

df_2022_2024_sample = df_2022_2024_sample.reset_index()

df_2022_2024_sample = df_2022_2024_sample.drop(columns='index')

df_2022_2024_sample.head(3)

df_2022_2024.to_csv('review_brimo_reclean_22_24.csv', index=False)
shutil.copy('/content/review_brimo_reclean_22_24.csv', '/content/drive/My Drive/TA/review_brimo_reclean_22_24.csv')
df_2022_2024_sample.to_csv('review_brimo_reclean_22_24_sample.csv', index=False)
shutil.copy('/content/review_brimo_reclean_22_24_sample.csv', '/content/drive/My Drive/TA/review_brimo_reclean_22_24_sample.csv')

"""## Data Labeling"""

df = pd.read_csv('/content/drive/My Drive/TA/review_brimo_reclean_22_24_sample.csv')
df.head(3)

df.info()

df = df.dropna(subset=['stemmingText'], axis=0)

df = df.reset_index()

df = df.drop(columns=['index', 'level_0'])
df.head(3)

df.isnull().sum()

"""### Aspek Labeling"""

import json
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

system_keyword = pd.DataFrame()
service_keyword = pd.DataFrame()
comfort_keyword = pd.DataFrame()

with open('/content/drive/MyDrive/TA/keyword.json', 'r') as json_data:
    keyword_dict = json.load(json_data)
    system_keyword['system'] = pd.DataFrame(keyword_dict['system'])
    service_keyword['service'] = pd.DataFrame(keyword_dict['service'])
    comfort_keyword['comfort'] = pd.DataFrame(keyword_dict['comfort'])

system_keyword_list = system_keyword['system'].values.tolist()
service_keyword_list = service_keyword['service'].values.tolist()
comfort_keyword_list = comfort_keyword['comfort'].values.tolist()

print(f'kerword system: {system_keyword_list}')
print(f'kerword service: {service_keyword_list}')
print(f'kerword comfort: {comfort_keyword_list}')

keyword_list = system_keyword_list + service_keyword_list + comfort_keyword_list
# Combine document texts and keywords for TF-IDF processing
corpus = keyword_list + df['stemmingText'].tolist()

print(f'corpus: {corpus}')

# Initialize a TfidfVectorizer
vectorizer = TfidfVectorizer()
# Compute TF-IDF matrix
tfidf_matrix = vectorizer.fit_transform(corpus)

# Split keyword vectors and document vectors
system_vector = tfidf_matrix[:len(system_keyword_list)]
service_vector = tfidf_matrix[len(system_keyword_list):len(system_keyword_list) + len(service_keyword_list)]
comfort_vector = tfidf_matrix[len(system_keyword_list) + len(service_keyword_list): len(system_keyword_list) + len(service_keyword_list) + len(comfort_keyword_list)]
# keyword_vectors = tfidf_matrix[:len(keyword_list)]
document_vectors = tfidf_matrix[len(keyword_list):]

# Compute similarities and label documents
results = []
for idx, doc_vector in enumerate(document_vectors):
    # Compute cosine similarity with each keyword category
    system_similarity = cosine_similarity(system_vector, doc_vector)
    service_similarity = cosine_similarity(service_vector, doc_vector)
    comfort_similarity = cosine_similarity(comfort_vector, doc_vector)

    # Aggregate scores for each category (e.g., mean similarity)
    avg_system_similarity = system_similarity.mean()
    avg_service_similarity = service_similarity.mean()
    avg_comfort_similarity = comfort_similarity.mean()

    # Determine the category with the highest similarity
    category_scores = {
        "system": avg_system_similarity,
        "service": avg_service_similarity,
        "comfort": avg_comfort_similarity,
    }
    best_category = max(category_scores, key=category_scores.get)

    # Store results
    results.append({
        "text2": df.loc[idx, "stemmingText"],
        "category": best_category,
        "scores": category_scores,
    })

# Convert results to a DataFrame for easier analysis
result_df = pd.DataFrame(results)
result_df.head()

result_df['scores'] = result_df['scores'].apply(lambda x: str(x))
def split_similarities(row):
    similarities_str = row['scores']  # Assuming 'similarities' column contains the string
    try:
        similarities_dict = eval(similarities_str) # Safely evaluate the string as a dictionary
        system_score = similarities_dict.get('system', 0) # Handle missing keys with default value 0
        service_score = similarities_dict.get('service', 0)
        comfort_score = similarities_dict.get('comfort', 0)

        return pd.Series({'system': 1 if system_score > 0 else 0,
                          'service': 1 if service_score > 0 else 0,
                          'comfort': 1 if comfort_score > 0 else 0})

    except (SyntaxError, NameError, TypeError) as e:
        print(f"Error processing row: {row}, Error: {e}")
        return pd.Series({'system': 0, 'service': 0, 'comfort': 0})  # Return default values on error

# Apply the function to create the new columns
new_columns = result_df.apply(split_similarities, axis=1)
result_df = pd.concat([result_df, new_columns], axis=1)

# Now result_df has 'system', 'service', and 'comfort' columns
result_df.head()

# Filter data by save only data with aspek
result_df = result_df[(result_df['system'] > 0) | (result_df['service'] > 0) | (result_df['comfort'] > 0)]
result_df.head()

"""Visualize Result"""

palette_color = sns.color_palette('viridis_r')
plt.figure(figsize=(12, 6))
plt.subplot(1, 3, 1)
plt.pie(result_df['system'].value_counts(), labels=result_df['system'].value_counts().index, autopct='%.0f%%', colors=palette_color)
plt.title('Proporsi Label System')

plt.subplot(1, 3, 2)
plt.pie(result_df['service'].value_counts(), labels=result_df['service'].value_counts().index, autopct='%.0f%%', colors=palette_color)
plt.title('Proporsi Label Service')

plt.subplot(1, 3, 3)
plt.pie(result_df['comfort'].value_counts(), labels=result_df['comfort'].value_counts().index, autopct='%.0f%%', colors=palette_color)
plt.title('Proporsi Label Comfort')

plt.tight_layout()
plt.savefig('aspect_label_distribution.png', dpi=1200, transparent=True, format='png')
plt.show()

df = pd.concat([df, result_df], axis=1)
df.head(2)

df = df.drop(columns=['text2'])

name = 'review_brimo_22_24_sample_aspect_label_cosine'
df.to_csv(f'{name}.csv', index=False)
shutil.copy(f'{name}.csv', f"/content/drive/My Drive/TA/{name}.csv")

"""### Sentimen Labeling"""

df_2022_2024_sample = pd.read_csv('/content/drive/My Drive/TA/review_brimo_22_24_sample_aspect_label_cosine.csv')
df_2022_2024_sample

"""Visualize Data Score"""

palette_color = sns.color_palette('viridis_r')
plt.figure(figsize=(10, 6))
plt.bar(df_2022_2024_sample['score'].value_counts().index, df_2022_2024_sample['score'].value_counts().values, color=palette_color)
plt.xlabel('Score')
plt.ylabel('Count')
plt.title('Bar Chart of Review Scores')
plt.show()

"""Labeling Sentimen By Score"""

df_2022_2024_sample['sentiment_by_score'] = 0
df_2022_2024_sample.loc[df_2022_2024_sample['score'] >= 4, 'sentiment_by_score'] = 1  # Score 5 or 4 gets sentiment_by_score 1
df_2022_2024_sample.loc[df_2022_2024_sample['score'] <= 3, 'sentiment_by_score'] = 0  # Score 1 or 3 gets sentiment_by_score 0

"""Visualize Result"""

plt.figure(figsize=(10, 6))
plt.bar(df_2022_2024_sample['sentiment_by_score'].value_counts().index, df_2022_2024_sample['sentiment_by_score'].value_counts().values, color=palette_color)
plt.xlabel('sentiment_by_score')
plt.ylabel('Count')
plt.title('Bar Chart of sentiment_by_score')
plt.xticks(df_2022_2024_sample['sentiment_by_score'].unique(), ['Positif(1)', 'Negatif(0)'])
plt.show()

plt.figure(figsize=(10, 6))
plt.pie(df_2022_2024_sample['sentiment_by_score'].value_counts(), labels=['Positif', 'Negatif'], autopct='%.0f%%', colors=palette_color)
plt.title('Proporsi Label Sentimen')
plt.savefig('sentiment_label_proporsi.png', dpi=1200, transparent=True, format='png')
plt.show()

"""# Klasifikasi Aspek

Import library
"""

!pip install keras-tuner --upgrade
import keras_tuner
from keras_tuner.tuners import RandomSearch
from keras_tuner.tuners import BayesianOptimization
from keras_tuner.engine.hyperparameters import HyperParameters
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau
import tensorflow as tf
import keras as K
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, BatchNormalization, Dropout, TextVectorization
from tensorflow.keras.utils import plot_model
from tensorflow.keras.preprocessing import text, sequence
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score, roc_curve, auc, roc_auc_score
from sklearn.preprocessing import label_binarize

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

"""## Function Utils CNN"""

def plot_confusion_matrix_aspect(y_true, x_val, model, model_name):
  y_pred = model.predict(x_val, batch_size = 32)
  y_pred = np.argmax(y_pred, axis=1)
  y_true = np.argmax(y_true, axis=1)
  cm = confusion_matrix(y_true, y_pred)
  plt.figure(figsize=(8, 6))
  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
              xticklabels=['System', 'Service', 'Comfort'],
              yticklabels=['System', 'Service', 'Comfort'])
  plt.xlabel('Predicted')
  plt.ylabel('True')
  plt.title(f'Confusion Matrix of {model_name}')
  plt.savefig(f'confusion_matrix_{model_name}_aspect_cosine.png', transparent=True, dpi=1200, format='png',)
  plt.show()

def plot_roc_auc_aspect(y_true, x_val, model, model_name):
  y_pred = model.predict(x_val, batch_size = 32)
  y_pred_probs =  np.exp(y_pred) / np.sum(np.exp(y_pred), axis=1, keepdims=True)
  classes = [0, 1, 2]
  y_true_bin = label_binarize(np.argmax(y_true.values, axis=1), classes=classes)
  n_classes = y_true_bin.shape[1]
  roc_auc_score(y_true_bin, y_pred_probs, multi_class='ovo', average='macro')
  fpr = dict()
  tpr = dict()
  roc_auc = dict()
  for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_probs[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

  # Compute micro-average ROC curve and ROC area
  fpr["micro"], tpr["micro"], _ = roc_curve(y_true_bin.ravel(), y_pred_probs.ravel())
  roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

  # Plot ROC curves for each class and the micro-average
  plt.figure()
  lw = 2
  plt.plot(fpr[0], tpr[0], color='darkorange',
          lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[0])
  plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
  plt.xlim([0.0, 1.0])
  plt.ylim([0.0, 1.05])
  plt.xlabel('False Positive Rate')
  plt.ylabel('True Positive Rate')
  plt.title(f'ROC Curve of Model {model_name}')
  plt.legend(loc="lower right")
  plt.savefig(f'roc_auc_curve_{model_name}.png', transparent=True, dpi=1200, format='png',)
  plt.show()

def plot_string_lengths(df, column_name):
  """Plots the length of each string in a specified column of a DataFrame.

  Args:
      df: The input DataFrame.
      column_name: The name of the column containing the strings.
  """
  string_lengths = df[column_name].astype(str).str.len()
  plt.figure(figsize=(10, 6))
  plt.hist(string_lengths, bins=50)  # Adjust bins as needed
  plt.xlabel('String Length')
  plt.ylabel('Frequency')
  plt.title(f'Distribution of String Lengths in "{column_name}"')
  plt.show()

def plot_learning_curve(history_train, model_name):
  plt.plot(history_train.history['accuracy'])
  plt.plot(history_train.history['val_accuracy'])
  plt.title(f'Learning Curve of Model {model_name}')
  plt.ylabel('Accuracy')
  plt.xlabel('Epoch')
  plt.legend(['Train', 'Validation'], loc='upper left')
  plt.savefig(f'learning_curve_{model_name}.png', transparent=True, dpi=1200, format='png',)
  plt.show()

def plot_learning_loss(history_train, model_name):
  plt.plot(history_train.history['loss'])
  plt.plot(history_train.history['val_loss'])
  plt.title(f'Model {model_name} loss')
  plt.ylabel('Loss')
  plt.xlabel('Epoch')
  plt.legend(['Train', 'Validation'], loc='upper right')
  plt.savefig(f'loss_curve_{model_name}.png', transparent=True, dpi=1200, format='png',)
  plt.show()

def plot_data(dataframe, column_name):
  dataframe[column_name].value_counts().plot(kind='bar')

def save_log_search(source_dir, destination_dir):
  try:
    copy_tree(source_dir, destination_dir)
    print(f"Successfully copied '{source_dir}' to '{destination_dir}'")
  except Exception as e:
    print(f"Error copying directory: {e}")

def plot_keras_model(model, model_name):
  plot_model(model, to_file=f'{model_name}.png', show_shapes=True, show_layer_names=True)

def classification_report_aspect(X, y, model, model_name):
  pred = model.predict(X, batch_size = 32)
  pred = np.argmax(pred, axis=1)
  y = np.argmax(y, axis=1)
  report = classification_report(y, pred, target_names=['system', 'service', 'comfort'])
  with open(f'classification_report_{model_name}.txt', 'w') as f:
    f.write(report)
  return report

"""## Read and Split Dataset"""

file_name = 'review_brimo_22_24_sample_aspect_label_cosine'
file_path = f'/content/drive/MyDrive/TA/{file_name}.csv'

df = pd.read_csv(file_path)
df.isnull().sum()

df_new = df[['stemmingText', 'system', 'service', 'comfort']].dropna(subset=['system', 'service', 'comfort'])
df_new.head()

y = df_new[['system', 'service', 'comfort']]
x = df_new['stemmingText']

tf.keras.utils.to_categorical(
    y, num_classes=3
)

x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)

x_val, x_test, y_val, y_test = train_test_split(x_val, y_val, test_size=0.5, random_state=42)

print("Shape of x_train:", x_train.shape)
print("Shape of x_val:", x_val.shape)
print("Shape of x_test:", x_test.shape)

print("Shape of y_trian", y_train.shape)
print("Shape of y_val", y_val.shape)
print("Shape of y_test", y_test.shape)

"""## TF-IDF"""

max_token = 10000
tfid_vec_layer = TextVectorization(
    max_tokens = max_token,
    standardize = 'lower_and_strip_punctuation',
    split = 'whitespace',
    output_mode = 'tf_idf',
    pad_to_max_tokens = False
)

tfid_vec_layer.adapt(x_train)

x_train_tfidf = tfid_vec_layer(x_train)
x_val_tfidf = tfid_vec_layer(x_val)
x_test_tfidf = tfid_vec_layer(x_test)

x_train_tfidf[:,:10]

print(f'vocabulary on tfid :{tfid_vec_layer.get_vocabulary()}')

vocab_size_tfidf = tfid_vec_layer.vocabulary_size()
print(f'vocabulary size on tfid :{vocab_size_tfidf}')

"""## Glove

### Int Index Text
"""

max_token = 10000
int_vec_layer = TextVectorization(
    max_tokens = max_token,
    standardize = 'lower_and_strip_punctuation',
    split = 'whitespace',
    output_mode = 'int',
    pad_to_max_tokens = True,
    output_sequence_length = 500
)

int_vec_layer.adapt(x_train)

x_train_int = int_vec_layer(x_train)
x_val_int = int_vec_layer(x_val)
x_test_int = int_vec_layer(x_test)

voc = int_vec_layer.get_vocabulary()
vocab_size_int = len(voc)
word_index = dict(zip(voc, range(len(voc))))

print(f'vocabulary size of int :{vocab_size_int}')
print("length of x_train_int:", len(x_train_int))
print("shape of x_train_int:", x_train_int.shape)
print("shape of x_val_int:", x_val_int.shape)
print("shape of x_val_int:", x_val_int.shape)

"""### glove download"""

!wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip glove*.zip

# load the whole embedding into memory
embeddings_index = dict()
f = open('/content/glove.6B.300d.txt')
for line in f:
	values = line.split()
	word = values[0]
	coefs = np.asarray(values[1:], dtype='float32')
	embeddings_index[word] = coefs
f.close()
print('Loaded %s word vectors.' % len(embeddings_index))

# create a weight matrix for words in training docs
embedding_matrix = np.zeros((vocab_size_int, 300))
for word, i in word_index.items():
	embedding_vector = embeddings_index.get(word)
	if embedding_vector is not None:
		embedding_matrix[i] = embedding_vector

"""## Callbacks"""

def allCallback(weights_file, patience):
  return [
      EarlyStopping(monitor='val_loss', patience=patience, verbose=1),
      ModelCheckpoint(weights_file, monitor='val_loss', save_weights_only=False, save_best_only=True, verbose=1),
      ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=patience//2, verbose=1),
      TensorBoard(log_dir='logs')
  ]

"""## Modelling CNN

### Hyperparameter Manual

#### Non Glove
"""

Model_CNN_NonGlove = Sequential()
Model_CNN_NonGlove.add(Embedding(input_dim=vocab_size_tfidf, output_dim=300, trainable=True))
Model_CNN_NonGlove.add(Conv1D(filters=128, kernel_size=3, strides=1, padding='same', activation='relu'))
Model_CNN_NonGlove.add(MaxPooling1D(pool_size=2))
Model_CNN_NonGlove.add(Conv1D(filters=128, kernel_size=3, strides=1, padding='same', activation='relu'))
Model_CNN_NonGlove.add(MaxPooling1D(pool_size=2))
Model_CNN_NonGlove.add(Conv1D(filters=128, kernel_size=3, strides=1, padding='same', activation='relu'))
Model_CNN_NonGlove.add(MaxPooling1D(pool_size=2))
Model_CNN_NonGlove.add(Flatten())
Model_CNN_NonGlove.add(Dense(units=128, activation='relu'))
Model_CNN_NonGlove.add(Dense(units=3, activation='sigmoid'))

Model_CNN_NonGlove.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
Model_CNN_NonGlove.summary()

hist_model_nonbayes = Model_CNN_NonGlove.fit(x=x_train_tfidf, y=y_train, validation_data=(x_val_tfidf, y_val), epochs=35,
                   callbacks=allCallback('model_cnn_tfidf_manual.keras',
                                            patience=10))

Model_CNN_NonGlove.evaluate(x_val_tfidf, y_val)

plot_keras_model(Model_CNN_NonGlove, 'CNN Non-Glove TF-IDF Manual')

Model_CNN_NonGlove.save('CNN_TF-IDF_Manual.keras')

plot_learning_curve(hist_model_nonbayes, 'CNN TF-IDF Manual')

plot_learning_loss(hist_model_nonbayes, 'CNN TF-IDF Manual')

plot_confusion_matrix_aspect(y_val, x_val_tfidf, Model_CNN_NonGlove, 'CNN TF-IDF Manual')

from sklearn.metrics import classification_report
pred = Model_CNN_NonGlove.predict(x_val_tfidf, batch_size = 32)
pred = np.argmax(pred, axis=1)
y_test_1 = np.argmax(y_val, axis=1)
report = classification_report(y_test_1, pred, target_names=['system', 'service', 'comfort'])
print(report)

with open('classification_report_CNN TF-IDF Manual.txt', 'w') as f:
  f.write(report)

dir_now = '/content'
dir_tujuan = '/content/drive/My Drive/TA/exp_11_12/CNN-NONGLOVE-MANUAL/'
ext = ('.keras', '.png', '.txt')
for file in os.listdir(dir_now):
  if file.endswith(tuple(ext)):
    src_path = os.path.join(dir_now, file)
    if os.path.exists(src_path):
      shutil.copy(src_path, dir_tujuan)
    else:
      print(f"File not found: {src_path}")
  else:
    continue

shutil.copy('/content/classification_report_CNN TF-IDF Manual.txt', '/content/drive/My Drive/TA/exp_11_12/CNN-NONGLOVE-MANUAL/')

"""#### Glove"""

Model_CNN_Glove = Sequential()
Model_CNN_Glove.add(Embedding(input_dim=vocab_size_int, output_dim=300, weights=[embedding_matrix], trainable=False))
Model_CNN_Glove.add(Conv1D(filters=128, kernel_size=3, activation='relu'))
Model_CNN_Glove.add(MaxPooling1D(pool_size=2))
Model_CNN_Glove.add(Conv1D(filters=128, kernel_size=3, activation='relu'))
Model_CNN_Glove.add(MaxPooling1D(pool_size=2))
Model_CNN_Glove.add(Conv1D(filters=128, kernel_size=3, activation='relu'))
Model_CNN_Glove.add(MaxPooling1D(pool_size=2))
Model_CNN_Glove.add(Flatten())
Model_CNN_Glove.add(Dense(units=128, activation='relu'))
Model_CNN_Glove.add(Dense(units=3, activation='sigmoid'))

Model_CNN_Glove.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
Model_CNN_Glove.summary()

hist_model_glove = Model_CNN_Glove.fit(x=x_train_int, y=y_train, validation_data=(x_val_int, y_val), epochs=35,
                   callbacks=allCallback('cnn_glove_manual.keras',
                                            patience=10))

plot_learning_curve(hist_model_glove, 'CNN Glove Manual')

plot_learning_loss(hist_model_glove, 'CNN Glove Manual')

Model_CNN_Glove = tf.keras.models.load_model('/content/drive/My Drive/TA/exp_11_12/CNN-NONGLOVE-MANUAL/CNN_Glove_Manual.keras')

plot_confusion_matrix_aspect(y_test, x_test_int, Model_CNN_Glove, 'CNN Glove Manual')

plot_roc_auc_aspect(y_test, x_test_int, Model_CNN_Glove, 'CNN Glove Manual')

plot_keras_model(Model_CNN_Glove, 'CNN Glove Manual')

print(classification_report_aspect(x_test_int, y_test, Model_CNN_Glove, 'CNN Glove Manual'))

Model_CNN_Glove.save('CNN_Glove_Manual.keras')

dir_now = '/content'
dir_tujuan = '/content/drive/My Drive/TA/exp_11_12/CNN-NONGLOVE-MANUAL/'
ext = ('.keras', '.png', '.txt')
for file in os.listdir(dir_now):
  if file.endswith(tuple(ext)):
    src_path = os.path.join(dir_now, file)
    if os.path.exists(src_path):
      shutil.copy(src_path, dir_tujuan)
    else:
      print(f"File not found: {src_path}")
  else:
    continue

"""### Hyperparameter Tuning

#### Non Glove
"""

class NonGloveHyperModel(keras_tuner.HyperModel):
    def build(self, hp, classes=3):
        model = Sequential()
        drop_rate = hp.Float("drop_rate", min_value=0.01, max_value=0.5, step=0.10)
        model.add(Embedding(input_dim=max_token, output_dim=300, trainable=True))
        for i in range(hp.Int('num_layers', min_value=2, max_value=4, step=1)):
          model.add(Conv1D(filters=hp.Int(name=f"filters_{i}", min_value=32, max_value=128, step=32),
                             kernel_size=hp.Int(name=f"kernel_{i}", min_value=3, max_value=5, step=2), strides=1,
                             padding='same', activation='relu'))
          if hp.Boolean("batch_norm"):
              model.add(BatchNormalization())
          model.add(MaxPooling1D(pool_size=1))
          if hp.Boolean('dropout'):
              model.add(Dropout(drop_rate))
        model.add(Flatten())
        for i in range(hp.Int('dense_layers', min_value=1, max_value=4, step=1)):
            model.add(Dense(units=hp.Int(name=f"units_{i}", min_value=32, max_value=128, step=32), activation='relu'))
            if hp.Boolean("batch_norm"):
                model.add(BatchNormalization())
            if hp.Boolean("dropout"):
                model.add(Dropout(drop_rate))
        model.add(Dense(3, activation='sigmoid'))

        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
        return model

    def fit(self, hp, model,x,*args, **kwargs):
        return model.fit(x, *args, shuffle=hp.Boolean("shuffle"), **kwargs)

class_=3
hp = HyperParameters()
hypermodel = NonGloveHyperModel()
cnn_hyperModel = hypermodel.build(hp, class_)
hypermodel.fit(hp, cnn_hyperModel,x=x_train_tfidf, y=y_train)
cnn_hyperModel.summary()



"""##### Hyperparameter Search: Bayesian Optimation"""

tuner_bayes_non_glove = BayesianOptimization(
    hypermodel= NonGloveHyperModel(),
    objective = 'val_accuracy',
    max_trials= 15,
    overwrite=True,
    directory='/content/bayes_aspect_cosine/',
    project_name='bayes_aspect_cosine')

tuner_bayes_non_glove.search(x=x_train_tfidf, y=y_train, epochs=10, validation_data=(x_val_tfidf, y_val), batch_size=32, callbacks=[TensorBoard("/tmp/tb_logs/bayes_aspect_cosine/")])

tuner_bayes_non_glove.results_summary()

best_hps_bayes_non_glove = tuner_bayes_non_glove.get_best_hyperparameters(3)

best_hps_bayes_non_glove[2].values

"""reload"""

tuner_bayes_non_glove = BayesianOptimization(
    hypermodel= NonGloveHyperModel(),
    objective = 'val_accuracy',
    max_trials= 15,
    overwrite=False,
    directory='/content/drive/My Drive/TA/exp_08_12/bayes_aspect_cosine/',
    project_name='bayes_aspect_cosine')

best_hps_bayes_non_glove = tuner_bayes_non_glove.get_best_hyperparameters(2)
best_hps_bayes_non_glove[0].values

"""##### Build Model & Training Model Hyper-1"""

cnn_model_non_glove_1 = NonGloveHyperModel()
cnn_tune_non_glove_bayes_1 = cnn_model_non_glove_1.build(best_hps_bayes_non_glove[0])
cnn_tune_non_glove_bayes_1.summary()

cnn_hist_tune_hyper1 = cnn_tune_non_glove_bayes_1.fit(x=x_train_tfidf, y=y_train, validation_data=(x_val_tfidf, y_val), epochs=35,
                    callbacks=allCallback('cnn_model_bayes_ng_tfidf_hyper_1.keras',
                                            patience=10))

plot_keras_model(cnn_tune_non_glove_bayes_1, 'CNN Non-Glove TF-IDF Post-Search Hyper-01')
cnn_tune_non_glove_bayes_1.save('cnn_model_bayes_non_glove_tfidf_hyper_01.keras')

"""##### Plot Learning & Loss Curve Hyper-01"""

plot_keras_model(cnn_tune_non_glove_bayes_1, 'CNN Non-Glove TF-IDF Post-Search Hyper-01')
cnn_tune_non_glove_bayes_1.save('cnn_model_bayes_non_glove_tfidf_hyper_01.keras')

plot_learning_loss(cnn_hist_tune_hyper1, 'CNN TF-IDF Non-GLOVE Hyper-01')

pred = cnn_tune_non_glove_bayes_1.predict(x_test_tfidf, batch_size = 32)
pred = np.argmax(pred, axis=1)
y_test_copy = np.argmax(y_test, axis=1)
report = classification_report(y_test_copy, pred, target_names=['system', 'service', 'comfort'])
print(report)

with open('classification_report_CNN_TF-IDF_Non-GLOVE_Hyper-01.txt', 'w') as f:
  f.write(report)

plot_confusion_matrix_aspect(y_test, x_test_tfidf, cnn_tune_non_glove_bayes_1, 'CNN TF-IDF Non-GLOVE Hyper-01')

plot_roc_auc_aspect(y_val, x_val_tfidf, cnn_tune_non_glove_bayes_1, 'CNN TF-IDF Non-GLOVE Hyper-01')

best_hps_bayes_non_glove[1].values

"""##### Build Model & Training Model Hyper-2"""

cnn_model_non_glove_2 = NonGloveHyperModel()
cnn_tune_non_glove_bayes_2 = cnn_model_non_glove_2.build(best_hps_bayes_non_glove[1])
cnn_tune_non_glove_bayes_2.summary()

cnn_hist_tune_hyper2 = cnn_tune_non_glove_bayes_2.fit(x=x_train_tfidf, y=y_train, validation_data=(x_val_tfidf, y_val), epochs=35,
                    callbacks=allCallback('cnn_model_bayes_ng_tfidf_hyper_2.keras',
                                            patience=10))

plot_keras_model(cnn_tune_non_glove_bayes_2, 'CNN Non-Glove TF-IDF Post-Search Hyper-02')
cnn_tune_non_glove_bayes_2.save('cnn_model_bayes_non_glove_tfidf_hyper_02.keras')

"""##### Plot Learning & Loss Curve Hyper-02"""

plot_learning_curve(cnn_hist_tune_hyper2, 'CNN TF-IDF Non-GLOVE Hyper-02')

plot_learning_loss(cnn_hist_tune_hyper2, 'CNN TF-IDF Non-GLOVE Hyper-02')

pred = cnn_tune_non_glove_bayes_2.predict(x_test_tfidf, batch_size = 32)
pred = np.argmax(pred, axis=1)
y_test_copy = np.argmax(y_test, axis=1)
report = classification_report(y_test_copy, pred, target_names=['system', 'service', 'comfort'])
print(report)

with open('classification_report_CNN_TF-IDF_Non-GLOVE_Hyper-02.txt', 'w') as f:
  f.write(report)

plot_confusion_matrix_aspect(y_test, x_test_tfidf, cnn_tune_non_glove_bayes_2, 'CNN TF-IDF Non-GLOVE Hyper-02')

plot_roc_auc_aspect(y_test, x_test_tfidf, cnn_tune_non_glove_bayes_2, 'CNN TF-IDF Non-GLOVE Hyper-02')

!mkdir -p drive/MyDrive/TA/exp_08_12/cnn_non_glove

import os
import shutil
dir_now = '/content'
dir_tujuan = '/content/drive/My Drive/TA/exp_08_12/cnn_non_glove/'
ext = ('.txt', '.keras', '.png')
for file in os.listdir(dir_now):
  if file.endswith(tuple(ext)):
    src_path = os.path.join(dir_now, file)
    if os.path.exists(src_path):
      shutil.copy(src_path, dir_tujuan)
    else:
      print(f"File not found: {src_path}")
  else:
    continue



"""##### Build Model & Training Model Hyper-3"""



cnn_model_non_glove_3 = NonGloveHyperModel()
cnn_tune_non_glove_bayes_3 = cnn_model_non_glove_3.build(best_hps_bayes_non_glove[2])
cnn_tune_non_glove_bayes_3.summary()

cnn_hist_tune_hyper3 = cnn_tune_non_glove_bayes_3.fit(x=x_train_tfidf, y=y_train, validation_data=(x_val_tfidf, y_val), epochs=35,
                    callbacks=allCallback('cnn_model_bayes_ng_tfidf_hyper_3.keras',
                                            patience=10))

plot_keras_model(cnn_tune_non_glove_bayes_3, 'CNN Non-Glove TF-IDF Post-Search Hyper-03')
cnn_tune_non_glove_bayes_3.save('cnn_model_bayes_non_glove_tfidf_hyper_03.keras')

"""##### Plot Learning & Loss Curve Hyper-03"""

plot_learning_curve(cnn_hist_tune_hyper3, 'CNN TF-IDF Non-GLOVE Hyper-03')

plot_learning_loss(cnn_hist_tune_hyper3, 'CNN TF-IDF Non-GLOVE Hyper-03')

pred = cnn_tune_non_glove_bayes_3.predict(x_test_tfidf, batch_size = 32)
pred = np.argmax(pred, axis=1)
y_test_copy = np.argmax(y_test, axis=1)
report = classification_report(y_test_copy, pred, target_names=['system', 'service', 'comfort'])
print(report)

with open('classification_report_CNN_TF-IDF_Non-GLOVE_Hyper-03.txt', 'w') as f:
  f.write(report)

plot_confusion_matrix_aspect(y_test, x_test_tfidf, cnn_tune_non_glove_bayes_3, 'CNN TF-IDF Non-GLOVE Hyper-03')

plot_roc_auc_aspect(y_test, x_test_tfidf, cnn_tune_non_glove_bayes_3, 'CNN TF-IDF Non-GLOVE Hyper-03')

!mkdir -p drive/MyDrive/TA/exp_08_12/cnn_non_glove

import os
import shutil
dir_now = '/content'
dir_tujuan = '/content/drive/My Drive/TA/exp_08_12/cnn_non_glove/'
ext = ('.txt', '.keras', '.png')
for file in os.listdir(dir_now):
  if file.endswith(tuple(ext)):
    src_path = os.path.join(dir_now, file)
    if os.path.exists(src_path):
      shutil.copy(src_path, dir_tujuan)
    else:
      print(f"File not found: {src_path}")
  else:
    continue

shutil.copy('/content/CNN Non-Glove TF-IDF Post-Search Hyper-03.png', dir_tujuan)
shutil.copy('/content/CNN Non-Glove TF-IDF Post-Search Hyper-02.png', dir_tujuan)

model_hyper3 = tf.keras.models.load_model('/content/drive/My Drive/TA/exp_08_12/cnn_non_glove/cnn_model_bayes_non_glove_tfidf_hyper_03.keras')

model_hyper3.evaluate(x_test_tfidf, y_test)

"""#### Glove

##### BayesOptimizationSearch
"""

class GloveHyperModel(keras_tuner.HyperModel):
    def build(self, hp, classes=3):
        model = Sequential()
        drop_rate = hp.Float("drop_rate", min_value=0.01, max_value=0.5, step=0.10)
        model.add(Embedding(input_dim=vocab_size_int, output_dim=300, weights=[embedding_matrix], trainable=False))
        for i in range(hp.Int('num_layers', min_value=2, max_value=4, step=1)):
          model.add(Conv1D(filters=hp.Int(name=f"filters_{i}", min_value=16, max_value=128, step=16),
                             kernel_size=hp.Int(name=f"kernel_{i}", min_value=3, max_value=7, step=2), strides=1,
                             padding='same', activation='relu'))
          if hp.Boolean("batch_norm"):
              model.add(BatchNormalization())
          model.add(MaxPooling1D(pool_size=1))
          if hp.Boolean('dropout'):
              model.add(Dropout(drop_rate))
        model.add(Flatten())
        for i in range(hp.Int('dense_layers', min_value=1, max_value=4, step=1)):
            model.add(Dense(units=hp.Int(name=f"units_{i}", min_value=16, max_value=128, step=16), activation='relu'))
            if hp.Boolean("batch_norm"):
                model.add(BatchNormalization())
            if hp.Boolean("dropout"):
                model.add(Dropout(drop_rate))
        model.add(Dense(3, activation='sigmoid'))

        model.compile(optimizer='adam', loss='BinaryCrossentropy', metrics=['accuracy'])
        return model

    def fit(self, hp, model,x,*args, **kwargs):
        return model.fit(x, *args, shuffle=hp.Boolean("shuffle"), **kwargs)

tuner_bayes = BayesianOptimization(
    hypermodel= GloveHyperModel(),
    objective = 'val_accuracy',
    max_trials= 10,
    overwrite=True,
    directory='/content/cnn_tuner_bayes_glove_cosine/',
    project_name='cnn_tuner_bayes_glove_cosine')

tuner_bayes.search(x=x_train_int, y=y_train, epochs=10, validation_data=(x_val_int, y_val), batch_size=64, callbacks=[TensorBoard("/tmp/tb_logs/bayes_glove/")])

tuner_bayes.results_summary()

"""##### Train Model Bayes Optimazation

reload
"""

# tuner_bayes = BayesianOptimization(
#     hypermodel= GloveHyperModel(),
    # objective = 'val_accuracy',
#     max_trials= 10,
#     overwrite=False,
#     directory='/content/drive/MyDrive/TA/cosine_aspect_06_12',
#     project_name='cnn_tuner_bayes_glove_cosine')

best_hps_bayes = tuner_bayes.get_best_hyperparameters(3)

best_hps_bayes[1].values

cnn_model = GloveHyperModel()
cnn_tune_model_bayes = cnn_model.build(best_hps_bayes[0])
cnn_tune_model_bayes.summary()

cnn_hist_tune = cnn_tune_model_bayes.fit(x=x_train_int, y=y_train, validation_data=(x_val_int, y_val), epochs=35,
                    callbacks=allCallback('cnn_glove_bayes_cosine.keras',
                                            patience=5))

plot_keras_model(cnn_tune_model_bayes, 'CNN Bayes Glove')

"""##### Plot Learning Curve"""

plot_learning_curve(cnn_hist_tune, 'CNN GLOVE BO')

plot_learning_loss(cnn_hist_tune, 'CNN GLOVE BO')

pred = cnn_tune_model_bayes.predict(x_test_tfidf, batch_size = 32)
pred = np.argmax(pred, axis=1)
y_test_labels = y_test
y_test_labels = np.argmax(y_test_labels, axis=1)
print(y_test_labels.shape, pred.shape)
# print(y_test_labels[:5], pred[:5])
print(accuracy_score(y_test_labels, pred))

plot_confusion_matrix_aspect(y_test, x_test_int, cnn_tune_model_bayes, 'CNN Glove BO')

plot_roc_auc_aspect(y_test, x_test_int, cnn_tune_model_bayes, 'CNN Glove BO')

from sklearn.metrics import classification_report
pred = cnn_tune_model_bayes.predict(x_test_int, batch_size = 32)
pred = np.argmax(pred, axis=1)
y_test_1 = np.argmax(y_test, axis=1)
report = classification_report(y_test_1, pred, target_names=['system', 'service', 'comfort'])
print(report)

with open('classification_report_CNN_Glove_Bayes.txt', 'w') as f:
  f.write(report)

save_log_search('/content/cnn_tuner_bayes_glove_cosine/', '/content/drive/MyDrive/TA/exp_08_12/')

!mkdir -p drive/MyDrive/TA/exp_08_12/cnn_BO_Glove_16_12

file_list =[
    '/content/CNN Bayes Glove.png',
    '/content/classification_report_CNN_Glove_Bayes.txt',
    '/content/cnn_glove_bayes_cosine.keras',
    '/content/confusion_matrix_CNN Glove BO_aspect_cosine.png',
    '/content/learning_curve_CNN GLOVE BO.png',
    '/content/loss_curve_CNN GLOVE BO.png',
    '/content/roc_auc_curve_CNN Glove BO.png'
]
dir_tujuan = '/content/drive/My Drive/TA/exp_08_12/cnn_BO_Glove_16_12/'
for file in file_list:
  shutil.copy(file, dir_tujuan)

import os
import shutil
dir_now = '/content'
dir_tujuan = '/content/drive/My Drive/TA/exp_08_12/cnn_BO_Glove_16_12/'
ext = ('.txt', '.keras', '.png')
for file in os.listdir(dir_now):
  if file.endswith(tuple(ext)):
    src_path = os.path.join(dir_now, file)
    if os.path.exists(src_path):
      shutil.copy(src_path, dir_tujuan)
    else:
      print(f"File not found: {src_path}")
  else:
    continue

"""## Load Model CNN"""

CNN_manual = tf.keras.models.load_model('/content/drive/My Drive/TA/exp_11_12/CNN-NONGLOVE-MANUAL/CNN_TF-IDF_Manual.keras')
CNN_BO = tf.keras.models.load_model('/content/drive/My Drive/TA/exp_08_12/cnn_non_glove/cnn_model_bayes_ng_tfidf_hyper_3.keras')
CNN_glove_manual = tf.keras.models.load_model('/content/drive/My Drive/TA/exp_11_12/CNN-NONGLOVE-MANUAL/CNN_Glove_Manual.keras')
CNN_glove_BO = tf.keras.models.load_model('/content/drive/My Drive/TA/exp_08_12/cnn_BO_Glove_16_12/cnn_glove_bayes_cosine.keras')
models = {
    'CNN Manul' : CNN_manual,
    'CNN Bayes Optimization': CNN_BO,
}
models_glove = {
    'CNN Glove Manual' : CNN_glove_manual,
    'CNN Glove Bayes Optimization':CNN_glove_BO
}

x_test_tfidf.shape

CNN_manual.summary()

CNN_BO.summary()

CNN_glove_manual.summary()

CNN_manual.evaluate(x_test_tfidf, y_test)

CNN_BO.evaluate(x_test_tfidf, y_test)

CNN_glove_manual.evaluate(x_test_int, y_test)

CNN_glove_BO.evaluate(x_test_int, y_test)

def plot_multiple_roc_curve(y_test, X_test, models):
  for model_name, model in models.items():
      y_pred=model.predict(X_test) # predict the test data
  # Compute False postive rate, and True positive rate
      fpr, tpr, thresholds = roc_curve(y_test, model.predict(X_test))
  # Calculate Area under the curve to display on the plot
      auc = roc_auc_score(y_test,model.predict(X_test))
  # Now, plot the computed values
      plt.plot(fpr, tpr, label='%s AUC (area = %0.4f)' % (model_name, auc))
  # Custom settings for the plot
  plt.plot([0, 1], [0, 1],'r--')
  plt.xlim([0.0, 1.0])
  plt.ylim([0.0, 1.05])
  plt.xlabel('1-Specificity(False Positive Rate)')
  plt.ylabel('Sensitivity(True Positive Rate)')
  plt.title('Receiver Operating Characteristic')
  plt.legend(loc="lower right")
  plt.savefig('roc_curve_plot.png', transparent=True, dpi=1200, format='png',)
  plt.show()   # Display

def plot_roc_auc_aspect(y_true, x_test, model, model_name):
  y_pred = model.predict(x_test, batch_size = 32)
  y_pred_probs =  np.exp(y_pred) / np.sum(np.exp(y_pred), axis=1, keepdims=True)
  classes = [0, 1, 2]
  y_true_bin = label_binarize(np.argmax(y_true.values, axis=1), classes=classes)
  n_classes = y_true_bin.shape[1]
  roc_auc_score(y_true_bin, y_pred_probs, multi_class='ovo', average='macro')
  fpr = dict()
  tpr = dict()
  roc_auc = dict()
  for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_probs[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

  # Compute micro-average ROC curve and ROC area
  fpr["micro"], tpr["micro"], _ = roc_curve(y_true_bin.ravel(), y_pred_probs.ravel())
  roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

  # Plot ROC curves for each class and the micro-average
  plt.figure()
  lw = 2
  for i in range(n_classes):
    plt.plot(fpr[i], tpr[i],
          lw=lw, label=f'ROC curve of class {i} (area = %0.4f)' % roc_auc[i])
  plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
  plt.xlim([0.0, 1.0])
  plt.ylim([0.0, 1.05])
  plt.xlabel('False Positive Rate')
  plt.ylabel('True Positive Rate')
  plt.title(f'ROC Curve of Model {model_name}')
  plt.legend(loc="lower right")
  plt.savefig(f'roc_auc_curve_{model_name}.png', transparent=True, dpi=1200, format='png',)
  plt.show()

plot_roc_auc_aspect(y_test, x_test_tfidf, CNN_BO, 'CNN Bayes Optimization')

plot_roc_auc_aspect(y_test, x_test_tfidf, CNN_manual, 'CNN Manual')

plot_roc_auc_aspect(y_test, x_test_int, CNN_glove_BO, 'CNN Glove Bayes Optimization')

plot_roc_auc_aspect(y_test, x_test_int, CNN_glove_manual, 'CNN Glove Manual')

"""# Klasifikasi Sentimen"""

!pip install imbalanced-learn
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score, roc_curve, auc, roc_auc_score
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE, RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from sklearn.model_selection import learning_curve

df = pd.read_csv('/content/drive/My Drive/TA/review_brimo_22_24_sample_label_aspect_cosine_sentiment_byscore.csv')
df.head()

df.info()

df.isnull().sum()

df_new = df[['stemmingText', 'sentiment_by_score']].dropna(subset=['stemmingText'])
df_new.head()

df_new['sentiment_by_score'].value_counts().plot(kind='bar', color='#5ec962')

df_new.count()

"""## Function Utils SVM"""

def print_metrics(y_test, X_test, model, model_name):
  y_pred = model.predict(X_test)
  accuracy = accuracy_score(y_test, y_pred)
  precision_svm = precision_score(y_test, y_pred)
  recall_svm = recall_score(y_test, y_pred)
  f1_score_svm = f1_score(y_test, y_pred)
  print(f"Accuracy of {model_name}:", accuracy)
  print(f"Precision of {model_name}:", precision_svm)
  print(f"Recall of {model_name}:", recall_svm)
  print(f"F1 Score of {model_name}:", f1_score_svm)
  with open(f'{model_name}_all_metrics.txt', 'w') as f:
    f.write(f"Accuracy of {model_name}: {accuracy}\n")
    f.write(f"Precision of {model_name}: {precision_svm}\n")
    f.write(f"Recall of {model_name}: {recall_svm}\n")
    f.write(f"F1 Score of {model_name}: {f1_score_svm}\n")

def plot_confusion_matrix(y_test, X_test, model, model_name):
  y_pred = model.predict(X_test)
  cm = confusion_matrix(y_test, y_pred)
  plt.figure(figsize=(8, 6))
  sns.heatmap(cm, annot=True, fmt='d', cmap='viridis', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])
  plt.title(f'Confusion Matrix {model_name}')
  plt.xlabel('Predicted Label')
  plt.ylabel('True Label')
  plt.savefig(f'confusion_matrix_{model_name}.png', transparent=True, dpi=1200, format='png',)
  plt.show()

def roc_curve_plot(model, model_name, y_test, X_test):
  y_pred = model.predict(X_test)
  fpr, tpr, thresholds = roc_curve(y_test,  y_pred)
  auc = roc_auc_score(y_test, y_pred)

  plt.plot(fpr,tpr,color='darkorange', lw=2, label="AUC="+str(auc))
  plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
  plt.xlim([0.0, 1.0])
  plt.ylim([0.0, 1.05])
  plt.title(f'ROC Curve of {model_name}')
  plt.ylabel('True Positive Rate')
  plt.xlabel('False Positive Rate')
  plt.legend(loc=4)
  plt.savefig(f'roc_curve_plot_{model_name}.png', transparent=True, dpi=1200, format='png',)
  plt.show()

def save_classification_report(y_test, X_test, model, model_name):
  y_pred = model.predict(X_test)
  report = classification_report(y_test, y_pred, output_dict=True)
  df_temp = pd.DataFrame(report).transpose()
  with open(f'{model_name}_classification_report.txt', 'w') as f:
    f.write(str(df_temp))

def save_model(model, model_name):
  filename = f'{model_name}.sav'
  pickle.dump(model, open(filename, 'wb'))

def load_model(file_path):
  filename = f'{file_path}.sav'
  loaded_model = pickle.load(open(filename, 'rb'))
  return loaded_model

def plot_learning_curve_model(X, y, model, cv=5, model_name='SVM'):
  train_sizes, train_scores, test_scores = learning_curve(
      estimator=model,
      X=X,
      y=y,
      train_sizes=np.linspace(0.1, 1.0, 10),
      cv=cv,
      scoring='accuracy',
      n_jobs=-1
  )

  train_mean = np.mean(train_scores, axis=1)
  train_std = np.std(train_scores, axis=1)
  test_mean = np.mean(test_scores, axis=1)
  test_std = np.std(test_scores, axis=1)

  plt.plot(train_sizes, train_mean, label='Training score')
  plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)
  plt.plot(train_sizes, test_mean, label='Cross-validation score')
  plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1)

  plt.xlabel('Training examples')
  plt.ylabel('Score')
  plt.title(f'Learning Curve {model_name}')
  plt.legend(loc="best")
  plt.grid()
  plt.savefig(f'learning_curve_{model_name}.png', transparent=True, dpi=1200, format='png')
  plt.show()

"""## Split Dataset

## TF-IDF
"""

X, y = df_new['stemmingText'], df_new['sentiment_by_score']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Jumlah Data Train:", (len(X_train), len(y_train)))
print("Jumlah Data Test:", (len(X_test), len(y_test)))

def transform_data(X_train, X_test):
  vectorizer = TfidfVectorizer()
  X_train = vectorizer.fit_transform(X_train)
  X_test = vectorizer.transform(X_test)
  return X_train, X_test, vectorizer

X_tfidf, X_test_tfidf, vectorizer = transform_data(X_train, X_test)

with open('vectorizer.pkl', 'wb') as f:
  pickle.dump(vectorizer, f)

with open('tfidf-train.txt', 'wb') as f:
  pickle.dump(X_tfidf, f)

with open('tfidf-test.txt', 'wb') as f:
  pickle.dump(X_test_tfidf, f)

"""## Modeling SVM

<hr>

### SVM

Create new dataset
"""

X_train_SVM, X_test_SVM, vectorizer = transform_data(X_train, X_test)

"""Modeling SVM"""

model_svc = SVC(C=0.2, kernel='linear', random_state=123)
model_svc.fit(X_train_SVM, y_train)

plot_learning_curve_model(X_train_tfidf, y_train, model_svc, 5, 'SVM')

"""Testing with data test"""

print_metrics(y_test, X_test_SVM, model_svc, 'SVM')

"""Save model"""

save_model(model_svc, 'SVM')

"""<hr>

### OverSampling: SMOTE

Create new dataset and resampling data
"""

X_train_smote, X_test_smote, _ = transform_data(X_train, X_test)

oversample = SMOTE()
X_train_smote, y_train_smote = oversample.fit_resample(X_train_smote, y_train)

print("Jumlah Data Train Sebelum SMOTE:", (Counter(y_train)))
print("Jumlah Data Sesudah SMOTE:", (Counter(y_train_smote)))

"""Modeling SVM"""

model_smote = SVC(C=0.2, kernel='linear', random_state=123)
model_smote.fit(X_train_smote, y_train_smote)

plot_learning_curve_model(X_train_smote, y_train_smote, model_smote, 5, 'SVM SMOTE')

"""Testing with data test"""

print_metrics(y_test, X_test_smote, model_smote, 'SVM_SMOTE')

save_model(model_smote, 'SVM_SMOTE')

"""<hr>

### Oversampling: RandomOverSampling

Create new dataset and resampling data
"""

X_train_ros, X_test_ros, _ = transform_data(X_train, X_test)

overSample = RandomOverSampler(sampling_strategy='minority')
X_train_ros, y_train_ros = overSample.fit_resample(X_train_ros, y_train)

print("Jumlah Data Train Sebelum Random Oversampling:", (Counter(y_train)))
print("Jumlah Data Sesudah Random Oversampling:", (Counter(y_train_ros)))

"""Modeling SVM"""

model_ros = SVC(C=0.2, kernel='linear', random_state=123)
model_ros.fit(X_train_ros, y_train_ros)

plot_learning_curve_model(X_train_ros, y_train_ros, model_ros, 5, 'SVM Random OverSampling')

"""Testing with data test"""

print_metrics(y_test, X_test_ros, model_ros, 'SVM_RandomOverSampling')

"""Save model"""

save_model(model_ros, 'SVM_RandomOverSampling')

"""<hr>

### RandomUnderSampling

Create new dataset and resampling data
"""

X_train_rus, X_test_rus, _ = transform_data(X_train, X_test)

underSample = RandomUnderSampler(sampling_strategy='majority')
X_train_rus, y_train_rus = underSample.fit_resample(X_train_rus, y_train)

print("Jumlah Data Train Sebelum Random Undersampling:", (Counter(y_train)))
print("Jumlah Data Sesudah Random Undersampling:", (Counter(y_train_rus)))

"""Modeling SVM"""

model_rus = SVC(C=0.2, kernel='linear', random_state=123)
model_rus.fit(X_train_rus, y_train_rus)

plot_learning_curve_model(X_train_rus, y_train_rus, model_rus, 5, 'SVM Random UnderSampling')

"""Testing with data test"""

print_metrics(y_test, X_test_rus, model_rus, 'SVM_RandomUnderSampling')

"""Save model"""

save_model(model_rus, 'SVM_RandomUnderSampling')

"""## Classification Report"""

classification_report_svm = classification_report(y_test, model_svc.predict(X_test_SVM))
save_classification_report(y_test, X_test_SVM, model_svc, 'SVM')
print("Classification Report of SVM:")
print(classification_report_svm)

y_pred_smote = model_smote.predict(X_test_smote)
classification_report_svm_smote = classification_report(y_test, y_pred_smote)
save_classification_report(y_test, X_test_smote, model_smote, 'SVM_SMOTE')
print("Classification Report of SVM_SMOTE:")
print(classification_report_svm_smote)

y_pred_ros = model_ros.predict(X_test_ros)
classification_report_svm_roc = classification_report(y_test, y_pred_ros)
save_classification_report(y_test, X_test_ros, model_ros, 'SVM_RandomOverSampling')
print("Classification Report of SVM_RandomOverSampling:")
print(classification_report_svm_roc)

y_pred_rus = model_rus.predict(X_test_rus)
classification_report_svm_rus = classification_report(y_test, y_pred_rus)
save_classification_report(y_test, X_test_rus, model_rus, 'SVM_RandomUnderSampling')
print("Classification Report of SVM_RandomUnderSampling:")
print(classification_report_svm_rus)

"""## Confusion Matrix SVM"""

plot_confusion_matrix(y_test, X_test_SVM, model_svc, 'SVM')

plot_confusion_matrix(y_test, X_test_smote, model_smote, 'SVM_SMOTE')

plot_confusion_matrix(y_test, X_test_ros, model_ros, 'SVM_RandomOverSampling')

plot_confusion_matrix(y_test, X_test_rus, model_rus, 'SVM_RandomUnderSampling')

"""## ROC-CURVE"""

roc_curve_plot(model_svc, 'SVM', y_test, X_test_SVM)

roc_curve_plot(model_smote, 'SVM_SMOTE', y_test, X_test_smote)

roc_curve_plot(model_ros, 'SVM_RandomOverSampling', y_test, X_test_ros)

roc_curve_plot(model_rus, 'SVM-RandomUnderSampling', y_test, X_test_SVM)

"""## Save Model SVM Result To Drive"""

!ls

import os
import shutil
dir_now = '/content'
dir_tujuan = '/content/drive/My Drive/TA/exp_08_12/svm_file/'
ext = ('.txt', '.sav', '.png', 'pkl')
for file in os.listdir(dir_now):
  if file.endswith(tuple(ext)):
    src_path = os.path.join(dir_now, file)
    if os.path.exists(src_path):
      shutil.copy(src_path, dir_tujuan)
    else:
      print(f"File not found: {src_path}")
  else:
    continue

"""<hr>

## Load Model
"""

main_dir = '/content/drive/My Drive/TA/exp_08_12/svm_file/'
model_svc = load_model(f'{main_dir}SVM')
model_smote = load_model(f'{main_dir}SVM_SMOTE')
model_ros = load_model(f'{main_dir}SVM_RandomOverSampling')
model_rus = load_model(f'{main_dir}SVM_RandomUnderSampling')

accuracy_ = accuracy_score(y_test, model_svc.predict(X_test_SVM))
print("Accuracy SVM:",accuracy_)

accuracy_ = accuracy_score(y_test, model_smote.predict(X_test_SVM))
print('Accuracy SVM-SMOTE:',accuracy_)

accuracy_ = accuracy_score(y_test, model_ros.predict(X_test_SVM))
print('Accuracy SVM-RandomOverSamplin:',accuracy_)

accuracy_ = accuracy_score(y_test, model_rus.predict(X_test_SVM))
print('Accuracy SVM-RandomUnderSampling:',accuracy_)

"""<hr>

## Start Labeling Non Label Data
"""

df[['pred_svm', 'pred_smote', 'pred_ros', 'pred_rus']] = ''

X_new = df['content']
X_new = vectorizer.transform(X_new)

df['pred_svm'] = model_svc.predict(X_new)
df['pred_smote'] = model_smote.predict(X_new)
df['pred_ros'] = model_ros.predict(X_new)
df['pred_rus'] = model_rus.predict(X_new)

df.head()

df['major_pred'] = df[['pred_svm', 'pred_smote', 'pred_ros', 'pred_rus']].mode(axis=1)[0]

df.head()

df.to_csv('final_result_svm_pred.csv')

"""# Demo

tunjukin code,

List procedure demo
*   
*
"""

